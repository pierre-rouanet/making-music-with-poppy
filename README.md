This is a demo of the [Poppy robot](http://www.poppy-project.org) performing electronic music from the movements of its own body, by interaction with a human. Drum samples are triggered by the movements of the left foot (kick) and the head (snare). The movements of the left shoulder modulates the pitch of a synthesized sound as well as the cutoff frequency of a low-pass filter applied on it. The [grunt](http://en.wikipedia.org/wiki/Grunting_(tennis)) is triggered by the right hand movements.

This was done by four members of the [Flowers group](https://flowers.inria.fr/) at Inria Bordeaux: [Cl√©ment Moulin-Frier](https://twitter.com/Clement_MF_), [Pierre Rouanet](https://twitter.com/RouanetPierre), [Matthieu Lapeyre](https://twitter.com/matth_lapeyre) and [Steve Nguyen](http://s-nguyen.net/).

A forum post discussing past and future experiments is available [here](https://forum.poppy-project.org/t/poppy-in-a-musical-setup-please-share-your-ideas/395).
